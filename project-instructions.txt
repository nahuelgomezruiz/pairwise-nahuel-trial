# Task

The overarching goal is to build a system which can grade papers similar to how Edexia would in a real world context. This means

- Finetuning 1000 samples is unreasonable to expect from a single piece of assessment
- Maybe QWK may not completely represent all the aspects a teacher is looking for in terms of accuracy. Maybe teachers are extremely sensitive to giving students low grades, and on a subset of the data the AI performs poorly on that, which makes the inaccuracies very dramatic for the teacher

- You are to complete this tasks (https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/data?select=train.csv) using a LLM infrastructure.
- Your goal is to achieve the highest evaluation (quadratic weighted kappa) possible by optimizing both how you interpret the rubric and how you apply that rubric
- Instead of using Fine tune or ML techniques, this is more targeted towards agent orchestration and working with existing LLMS like o3.
- What they are given
    - A very basic starting point
        - JSON of just the rubric verbatim
        - A marking process that go criterion by criterion and pass the criterion and the whole assignment into o3 which then output a structured format of something like “reasoning” and “final mark”
        - Then a eval function that fund the marking model on a set data set, and output the IRR (kappens cappa or something)
    - Then some ideas and trip
        - Consider splitting up the holistic rubric into a analytical rubric
        - Consider marking a few your self
        - Consider trying to understand the differences in interpretation of the rubric between Edexia and the human
        - Consider storing examples
        - Consider getting multiples AI to mark it

# Resources

https://github.com/Edexia/trial-template REPO

Fork the repository to begin working on it in your own profile. Copy env from 

[Env](https://www.notion.so/Env-20b509129597802bbd2af25104d52f68?pvs=21)

Read [README.md](http://README.md) in the repository template link above. These will be instructions on the repository itself. Feel free if you are comfortbale to start with your own, but the repository provides:

- Sheets integration to write data to https://docs.google.com/spreadsheets/d/1_y25dZeBuQabXbbphKt6oMzxQQ33AuTAe1ZBogUrpFg/edit?gid=718149133#gid=718149133
- Openai API simple grading agent setup which runs
- EVAL function
- Data downloaded already
- $500 of API credits - Nathan can check usage for you

### Tips:

- Understand HL overview of the codebase, consider rubric, submission, prompts, orchestration, and be able to mindmap it to understand generally the workflow and points of potential improvement.
- Run `python scripts/grade_essays.py` to get your baseline. Look at where the AI gets scores wrong and why. The key is continuous iteration: analyze failures → adjust your rubric/orchestration → test → repeat.
- Focus on splitting broad criteria into specific, measurable components that AI can evaluate consistently. Build multiple grading approaches (detailed vs holistic, comparison-based, error-focused) and A/B them.
- Track your accuracy metrics and only keep changes that actually improve performance.

### **First Principles**

- Good essay grading comes down to consistent application of clear criteria. Humans grade well when they have specific things to look for and concrete examples.
- AI is the same - it needs unambiguous instructions and well-defined standards.
- The rubric is your most important tool: make it so clear that any reasonable person (or AI) would give similar scores. If your AI gives inconsistent scores for similar essays, the rubric needs work.
- If it consistently misses certain types of errors, you need specialized detection. If multiple approaches disagree significantly, there's usually ambiguity in your criteria that needs resolving.